{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0o_pH41bp6z"
   },
   "source": [
    "# Machine Learning on Streams with Apache Beam\n",
    "\n",
    "Read-only link: [notebook](https://colab.research.google.com/drive/1RWtxEWsjzlrltbx4zrKMYOUt_P0KCkwA?usp=sharing)\n",
    "\n",
    "\n",
    "# What we will build \n",
    "\n",
    "In this workshop, we will learn how to use Apache Beam to detect potential disasters occurring in the world. We will receive a stream of Tweets as input, use machine learning to detect anomalies, and send alert events in an output stream. These alerts can be consumed in real-time by any service of interest.\n",
    "\n",
    "# Coding environment\n",
    "\n",
    "You simply need to install Apache Beam using `pip install apache-beam` to run this notebook. You also need pandas and scikit-learn.\n",
    "\n",
    "If you are using Noto, you can simply run these commands in your terminal\n",
    "\n",
    "```\n",
    "my_venvs_create dag\n",
    "my_venvs_activate dag\n",
    "pip install apache-beam\n",
    "my_kernels_create dag \"DAG\"\n",
    "```\n",
    "\n",
    "Then, switch your notebook kernel using the kernel selector on the top-right of this page. If it does not appear in the dropdown, close the window, shut down the notebook's kernel and re-open the notebook.\n",
    "\n",
    "# Streaming Data Processing\n",
    "\n",
    "Data processing is either offline or online. There are a lot of applications    from streaming/online machine learning, where you receive a continuous flux of data to transform. The output can then be either stored or directly sent to another output stream.\n",
    "For instance, detecting anomalies in a continuous stream of events (IoT sensors, transactions, ...) and sending alterts to an output stream.\n",
    "\n",
    "![Batch vs. Stream Processing](assets/batch-stream.png)\n",
    "\n",
    "\n",
    "# Apache Beam\n",
    "\n",
    "**Apache Beam** is a unified programming model to define batch and **streaming** data processing jobs. It's compatible with multiple executor engines such as *Apache Spark*, *Apache Flink* or *Google DataFlow*. It follows the philosopy \"Write once, run everywhere\" and has SDK in multiple languages, i.e **Python**, Java and Go.\n",
    "\n",
    "> Explain pipeline worker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WujCPEIqJnaL"
   },
   "source": [
    "\n",
    "## Assumptions\n",
    "\n",
    "We assume a basic knowledge in machine learning (mainly Pandas and SKlearn) and Python programming language.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQIgef4OJ4q9"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset comes from a Kaggle competition: [Natural Language Processing with Disaster Tweets\n",
    "](https://www.kaggle.com/c/nlp-getting-started).\n",
    "\n",
    "The objective is to predict which Tweets talks about about natural disasters and which does not.\n",
    "\n",
    "Example of disaster Tweet:\n",
    "> Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
    "\n",
    "> Forest fire near La Ronge Sask. Canada\n",
    "\n",
    "> All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
    "\n",
    "Example of normal Tweet:\n",
    "> What's up man?\n",
    "\n",
    "> I love fruits\n",
    "\n",
    "> Summer is lovely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m25DTXxUJ_A4"
   },
   "source": [
    "# Technologies\n",
    "1. **Google Cloud Storage**: bucket to store the data and the trained model\n",
    "2. **Apache Beam**: Python SDK to create data processing job\n",
    "3. **Google Cloud Pub/Sub**: ingestion platform for event-driven systems and streaming analytics\n",
    "4. **Google Cloud DataFlow**: cloud executor for job expressed with Apache Beam SDK\n",
    "\n",
    "> explain access google cloud, simulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TJBGkOrKE3a"
   },
   "source": [
    "\n",
    "# Overall System\n",
    "![overall pipeline](https://i.postimg.cc/sDSB2Tzm/overall-pipeline.png\n",
    "\"Overall System\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07PIO-DaKHrk"
   },
   "source": [
    "\n",
    "# Beam Pipeline\n",
    "\n",
    "![beam pipeline](https://i.postimg.cc/T1rRkTD9/beam-pipeline.png\n",
    "\"Beam Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytaY3jMyKQPj"
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. Setup libraries and configuration\n",
    "2. Train a simple model and save it in a Google Cloud bucket\n",
    "3. Prepare the input stream (Tweets)\n",
    "4. Build a Beam streaming pipeline to process the Tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5sXeRTiggvd"
   },
   "source": [
    "# Setup and Configuration\n",
    "Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "RWOZQnsF88-6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Iterable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znGc0CGh1YGl"
   },
   "source": [
    "# A glance at Google Cloud Pub/Sub\n",
    "\n",
    "It's the service used in this workshop to handle stream of events. We briefly introduce the 2 main concepts to understand how it works: **Topics** and **Subscriptions**.\n",
    "\n",
    "\n",
    "## Topics: enqueuing messages\n",
    "\n",
    "A topic can be seen as queue. It's the place where you push/enqueue messages. In our setup we want a topic to push Tweets (input) and a topic to push predictions (output).\n",
    "\n",
    "## Subscriptions: dequeuing (consuming) messages\n",
    "\n",
    "Now that messages are pushed to the topics (*queues*) we use subscriptions to pull messages out of it (*dequeue*). Any application who need to access message in the queue does it through a subscription. In our case we want a subscription to pull Tweets for the first queue and a subscription to pull predictions from the second queue.\n",
    "\n",
    "A topic can have multiple subscriptions. It will deliver the messages once to every subscriptions. \n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "![overall pipeline](https://i.postimg.cc/zvL1f2Rk/pub-sub.png\n",
    "\"Overall System\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MH7Y64xrjvKi"
   },
   "source": [
    "# Modeling\n",
    "\n",
    "We need a model to classify tweets as disaster or not. \n",
    "\n",
    "**The purpose of this workshop is not modeling, so we will not spend to much time on it and go for an easy solution. The pre-processing and modeling are purposedly rudimentary.**\n",
    "\n",
    "We build the model as a SKlearn pipeline, it has the advantage to bundle multiple steps into one estimator. We can then have our preprocessing included in the model:\n",
    "\n",
    "1. First we select the column of interest in the DataFrame, in our case the text column. This column does not contain any NA values so ...\n",
    "2. We vectorize the raw text using TFIDF ([TFIDF details](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), [Sklearn TFIDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))\n",
    "3. We apply a Random Forest classifier ([Sklearn RandomForestClassfier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Here are two links for the train and test datasets:\n",
    "\n",
    "- [Train dataset](https://drive.google.com/file/d/1rsrAu4F13UCHsKpWjxRIh0ObsjWSMVSE/view?usp=sharing)\n",
    "- [Test dataset](https://drive.google.com/file/d/1yjX4e2U2auLQn01HYUgBKEJT8q3g15BJ/view?usp=sharing)\n",
    "\n",
    "Download them and place them in the sames folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "g41_Y6Tw1Yg6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "collapsed": true,
    "id": "wRiNljO_JLM_",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d30129ad-040f-467e-926d-4d7972fbfc52"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                       location  \\\n",
       "31  48  ablaze                     Birmingham   \n",
       "32  49  ablaze  Est. September 2012 - Bristol   \n",
       "33  50  ablaze                         AFRICA   \n",
       "34  52  ablaze               Philadelphia, PA   \n",
       "35  53  ablaze                     London, UK   \n",
       "\n",
       "                                                 text  target  \n",
       "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "32  We always try to bring the heavy. #metal #RT h...       0  \n",
       "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "34                 Crying out for more! Set me ablaze       0  \n",
       "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from our Google Cloud bucket\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "df_train.dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppr3pUCkYxzI"
   },
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "5uR-BLx31lqm",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def select_text(df: pd.DataFrame) -> pd.Series:\n",
    "    return df[\"text\"]\n",
    "\n",
    "# Model pipeline\n",
    "pipe = Pipeline([\n",
    "    ('selector', FunctionTransformer(select_text)),\n",
    "    ('tfidf', TfidfVectorizer(lowercase=True)),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "collapsed": true,
    "id": "UxC7kcGf6EgF",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5a1343aa-8c32-485e-b799-4cd1be12b5b8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>train_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.404588</td>\n",
       "      <td>0.902972</td>\n",
       "      <td>0.764883</td>\n",
       "      <td>0.999892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.043009</td>\n",
       "      <td>0.358259</td>\n",
       "      <td>0.037528</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fit_time  score_time  test_roc_auc  train_roc_auc\n",
       "mean  41.404588    0.902972      0.764883       0.999892\n",
       "std    3.043009    0.358259      0.037528       0.000082"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTIONAL - cross-validation step to get an idea of model performances\n",
    "\n",
    "cv_results = cross_validate(pipe, df_train, df_train['target'], cv=3, scoring=[\"roc_auc\"], return_train_score=True, n_jobs=-1)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results.agg((\"mean\", \"std\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "baIg4s8s_fuN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = pipe.fit(df_train, df_train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "collapsed": true,
    "id": "bHov_EhOl0dQ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e9a997bb-0100-4d66-9b8f-5edbc269fb3b"
   },
   "outputs": [],
   "source": [
    "# We pickle and save the model\n",
    "pickle.dump(model, open(\"model.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-3JqE8dcOWz"
   },
   "source": [
    "# Beam Pipeline Overview\n",
    "\n",
    "We can summarize beam pipelines as follow:\n",
    "\n",
    "1. Read data from any source into a `PCollection` A\n",
    "2. Transform A into another `PCollection` B by applying a `PTranform` (map, filter, ...)\n",
    "3. Transform B intot another `PCollection`\n",
    "4. ... keep chaining tranformers\n",
    "5. Save the final output somewhere\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![beam-pipeline-doc](https://beam.apache.org/images/design-your-pipeline-linear.svg \"Windowing\")\n",
    "[*Image taken from official documentation*](https://beam.apache.org/documentation/programming-guide/#transforms)\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Initialization\n",
    "\n",
    "We first need an pipeline. It will be the entry point:\n",
    "\n",
    "```python\n",
    "pipeline = beam.Pipeline()\n",
    "```\n",
    "\n",
    "Or to create a pipeline and run it locally directly:\n",
    "\n",
    "```python\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    ...\n",
    "```\n",
    "\n",
    "## Applying a transform on your pipeline\n",
    "\n",
    "Use the **pipe** operator `|` to apply a transform on a collection. You can pair it with the `>>` to give a name to the transformation:\n",
    "\n",
    "Without name:\n",
    "\n",
    "```python\n",
    "pipeline | transform\n",
    "```\n",
    "\n",
    "With name:\n",
    "\n",
    "```python\n",
    "pipeline | \"step_name\" >> transform\n",
    "```\n",
    "\n",
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "id": "sbzxmnoywjUF",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "VurOXSSLm27y",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ffddaa42-b857-4e28-e7b7-650eb6f25996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, "
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    (pipeline\n",
    "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvzwqOeVmjyD"
   },
   "source": [
    "\n",
    "## Common operators\n",
    "\n",
    "### beam.Map (map a function on a collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "DeLzIK1UlXod",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ae7f7a40-f4c3-483a-f1c0-5ca1bbdae32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, 3, 4, 5, 6, "
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    (pipeline\n",
    "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"add_one\" >> beam.Map(lambda x: x + 1)\n",
    "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcw2t0J8keRH"
   },
   "source": [
    "### beam.ParDo with beam.DoFn\n",
    "\n",
    "Beam has a generic transform for data processing: `beam.ParDo`. \n",
    "It takes as input a function to apply: `beam.DoFn`. \n",
    "\n",
    "### ParDo\n",
    "1. run on each element in the input `PCollection`\n",
    "2. apply a processing function\n",
    "3. emit **zero**, **one**, or **many outputs** for each input element\n",
    "\n",
    "It's generic and can implement any usual transform: map, flatmap, filter, ...\n",
    "Indeed all these are special cases of `beam.ParDo` processing.\n",
    "\n",
    "\n",
    "#### DoFn\n",
    "`beam.DoFn` represent a processing function applied by the `beam.ParDo`.\n",
    "\n",
    "A beam.DoFn function **should always return an iterable or None**. beam.ParDo will flatten the iterable.\n",
    "\n",
    "\n",
    "For instance if you want to return one element, you need to wrap it into a list or tuple:\n",
    "\n",
    "```\n",
    "1. 1 -> Pardo()\n",
    "2. DoFnMultBy2(1) -> [2]\n",
    "3. ParDo([2]) -> 2\n",
    "```\n",
    "\n",
    "The first dimension of your output is flattened. \n",
    "\n",
    "Another nice feature is **stateful computation**. Since `beam.DoFn` is a class, it is possible to have an **internal state**. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LwAxRlf9c9z"
   },
   "source": [
    "Quick example: add an incremental index, starting from N, to elements\n",
    "\n",
    "```\n",
    "Input: 1, 2, 3, 4, 5\n",
    "\n",
    "Apply IdxFn\n",
    "\n",
    "Output:  (N, 1), (N+1, 2), (N+2, 3), (N+3, 4), (N+4, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EnDVvN6lkevQ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "6e269a40-09e0-4229-dc4d-facec2c68d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like a Map:\n",
      "(10, 1), (11, 2), (12, 3), (13, 4), (14, 5), "
     ]
    }
   ],
   "source": [
    "class IdxFn(beam.DoFn):\n",
    "    \"\"\"Custom DoFn function to add an index. It will be applied using beam.ParDo\"\"\"\n",
    "    def __init__(self, init_state: int):\n",
    "        # this is is the first number to use as an index, we will increment it for each element\n",
    "        self.state = init_state\n",
    "\n",
    "    def process(self, elem: Any) -> Iterable[Any]:\n",
    "        # create a tuple with the current state as index and the element\n",
    "        res = (self.state, elem)\n",
    "        # increment the state\n",
    "        self.state += 1\n",
    "        return [res]\n",
    "\n",
    "print(\"Like a Map:\")\n",
    "with beam.Pipeline() as pipeline:\n",
    "    (pipeline\n",
    "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"with_idx\" >> beam.ParDo(IdxFn(init_state=10))\n",
    "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcLlyyyWlpIy"
   },
   "source": [
    "### beam.Filter (filter a collection) - Optional, not used in this workshop\n",
    "\n",
    "Write a similar pipeline that prints only the even numbers in a created list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "cVgVTUs5ipIz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b7ff7f81-d469-4a0a-a45b-cd5741fce473"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-15-0915f153c048>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-0915f153c048>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    ###\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    ### \n",
    "    ### YOUR CODE HERE\n",
    "    ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFMqrBAglrZ4"
   },
   "source": [
    "### beam.FlatMap (filter a collection) - Optional, not used in this workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "EIzW48URleVn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "23f28475-5f47-4b1e-eb72-cdfcd9244c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 1, 2, 2, 3, 3, 4, 4, 5, 5, "
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    (pipeline\n",
    "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"duplicate_inputs\" >> beam.FlatMap(lambda x: [x, x])\n",
    "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWSvhW3_j4xe"
   },
   "source": [
    "### beam.window.WindowInto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-XeoYLPwgDF"
   },
   "source": [
    "#### Without window\n",
    "\n",
    "![without-window](https://i.postimg.cc/N0Nksgrx/without-window.png\n",
    "\"Windowing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htCT4SRawmhf"
   },
   "source": [
    "#### With session window\n",
    "\n",
    "![with-window](https://i.postimg.cc/cLFWzjCV/with-window.png\n",
    "\"Windowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "nLu3MEbaerc5",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "2bf23fbd-b51b-4d83-ed8a-5448e344990d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: (Key(), [1, 2, 3])\n",
      "Batch: (Key(), [4])\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    input_stream = [(1, 100), (2, 101), (3, 102), (4, 200)]\n",
    "    input_stream = [beam.window.TimestampedValue(elem, timestamp) for elem, timestamp in input_stream]\n",
    "    pc = (pipeline\n",
    "        | beam.Create(input_stream)\n",
    "        | 'window' >> beam.WindowInto(beam.window.FixedWindows(5))\n",
    "        | 'group' >> beam.GroupBy()\n",
    "        | 'pp' >> beam.Map(lambda x: print(f\"Batch: {x}\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-9a_g1VI4U6"
   },
   "source": [
    "# Our Beam Pipeline\n",
    "\n",
    "As a reminder, this is what we want:\n",
    "\n",
    "![beam pipeline](https://i.postimg.cc/xT5TvZnJ/beam-pipeline-focus.png\n",
    "\"Beam Pipeline\")\n",
    "\n",
    "We will focus on two important parts: **Grouping by chunk** of time and **Predicting**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teGW6YTwJ3jy"
   },
   "source": [
    "# Predict\n",
    "\n",
    "The most custom and critical step here is the **prediction**. Indeed we need to build a custom `beam.DoFn` with an initialization step pulling the pickled model from Google Cloud and loading the model. Then use the model to transform the incoming tweets. \n",
    "\n",
    "Here are the steps for the prediction part:\n",
    "1. we implement a custom `beam.DoFn` class called `ApplyModel`\n",
    "2. in the `__init__` method we pull and load the model\n",
    "3. we implement the `process` method which receives batch of tweets and use the model to label them as *DISASTER* or *NORMAL*\n",
    "\n",
    "\n",
    "![dofn](https://i.postimg.cc/KYpdxdBT/dofn.png\n",
    "\"Windowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "id": "GRb9F-MPG9vp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'id': 0, 'keyword': nan, 'location': nan, 'text': 'Just happened a terrible car crash'}, 0), ({'id': 2, 'keyword': nan, 'location': nan, 'text': 'Heard about #earthquake is different cities, stay safe everyone.'}, 1), ({'id': 3, 'keyword': nan, 'location': nan, 'text': 'there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all'}, 2), ({'id': 9, 'keyword': nan, 'location': nan, 'text': 'Apocalypse lighting. #Spokane #wildfires'}, 3), ({'id': 11, 'keyword': nan, 'location': nan, 'text': 'Typhoon Soudelor kills 28 in China and Taiwan'}, 4), ({'id': 12, 'keyword': nan, 'location': nan, 'text': \"We're shaking...It's an earthquake\"}, 5), ({'id': 21, 'keyword': nan, 'location': nan, 'text': \"They'd probably still show more life than Arsenal did yesterday, eh? EH?\"}, 6), ({'id': 22, 'keyword': nan, 'location': nan, 'text': 'Hey! How are you?'}, 7), ({'id': 27, 'keyword': nan, 'location': nan, 'text': 'What a nice hat?'}, 8), ({'id': 29, 'keyword': nan, 'location': nan, 'text': 'Fuck off!'}, 9), ({'id': 30, 'keyword': nan, 'location': nan, 'text': \"No I don't like cold!\"}, 50), ({'id': 35, 'keyword': nan, 'location': nan, 'text': \"NOOOOOOOOO! Don't do that!\"}, 51), ({'id': 42, 'keyword': nan, 'location': nan, 'text': \"No don't tell me that!\"}, 52), ({'id': 43, 'keyword': nan, 'location': nan, 'text': 'What if?!'}, 53), ({'id': 45, 'keyword': nan, 'location': nan, 'text': 'Awesome!'}, 54), ({'id': 46, 'keyword': 'ablaze', 'location': 'London', 'text': \"Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham's Wholesale Market http://t.co/irWqCEZWEU\"}, 55), ({'id': 47, 'keyword': 'ablaze', 'location': \"Niall's place | SAF 12 SQUAD |\", 'text': '@sunkxssedharry will you wear shorts for race ablaze ?'}, 56), ({'id': 51, 'keyword': 'ablaze', 'location': 'NIGERIA', 'text': '#PreviouslyOnDoyinTv: Toke Makinwa\\x89Ûªs marriage crisis sets Nigerian Twitter ablaze... http://t.co/CMghxBa2XI'}, 57), ({'id': 58, 'keyword': 'ablaze', 'location': 'Live On Webcam', 'text': 'Check these out: http://t.co/rOI2NSmEJJ http://t.co/3Tj8ZjiN21 http://t.co/YDUiXEfIpE http://t.co/LxTjc87KLS #nsfw'}, 58), ({'id': 60, 'keyword': 'ablaze', 'location': 'Los Angeles, Califnordia', 'text': 'PSA: I\\x89Ûªm splitting my personalities.\\n\\n?? techies follow @ablaze_co\\n?? Burners follow @ablaze'}, 59)]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<apache_beam.transforms.window.TimestampedValue at 0x7f9ede46fef0>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46ff28>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46fcc0>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46fac8>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46f8d0>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46f6d8>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46f4a8>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46f278>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46f080>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46f9e8>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46fb70>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46fbe0>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46fd68>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46fdd8>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede46ff60>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede28d048>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede28d0b8>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede28d128>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede28d198>,\n",
       " <apache_beam.transforms.window.TimestampedValue at 0x7f9ede28d208>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_jsons = pd.read_csv(\"data/test.csv\").to_dict(\"records\")\n",
    "\n",
    "timestamps = list(range(0, 10)) + list(range(50, 60)) \n",
    "tweets_input_stream = list(zip(test_jsons[:len(timestamps)], timestamps))\n",
    "\n",
    "print(tweets_input_stream)\n",
    "\n",
    "for x, y in tweets_input_stream:\n",
    "\n",
    "    print(y)\n",
    "\n",
    "tweets_input_stream = [beam.window.TimestampedValue(elem, timestamp) for elem, timestamp in tweets_input_stream]\n",
    "\n",
    "tweets_input_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    pc = (pipeline\n",
    "        | beam.Create(tweets_input_stream)\n",
    "        | 'window' >> beam.WindowInto(beam.window.FixedWindows(10))\n",
    "        | 'group' >> beam.GroupBy()\n",
    "        | 'pp' >> beam.Map(lambda x: print(f\"Batch: {len(x[1])}\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model in Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ApplyModel(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the function\"\"\"\n",
    "        self._model = None\n",
    "        # We import in the init statement to have the library available even when we run the pipeline \n",
    "        # on remote executors such as DataFlow\n",
    "        import pandas as pd\n",
    "        import pickle\n",
    "        self._pd = pd\n",
    "        self._model = pickle.load(open(\"model.pkl\", 'rb'))\n",
    "\n",
    "     \n",
    "    def process(self, group: Tuple[Any, Iterable[Any]]) -> Iterable[Any]:\n",
    "        \"\"\"Process every batch of tweets\"\"\"\n",
    "        # extract batch and build DataFrame\n",
    "        \n",
    "        ### \n",
    "        ### YOUR CODE HERE\n",
    "        ###\n",
    "        \n",
    "        # predict\n",
    "        # modify \"prediction\" column in df as \"NORMAL\" for 0 and \"DISASTER\" for 1\n",
    "        # format DataFrame to JSON format in variable `res`\n",
    "        \n",
    "        ### \n",
    "        ### YOUR CODE HERE\n",
    "        ###\n",
    "    \n",
    "        return res\n",
    "\n",
    "\n",
    "def build_pipeline(pipeline: beam.Pipeline) -> beam.Pipeline:\n",
    "    \"\"\"Takes an empty beam.Pipeline in input and returns the full beam.Pipeline\"\"\"\n",
    "\n",
    "    parse = pipeline | beam.Create(tweets_input_stream)\n",
    "\n",
    "    \n",
    "    # group the tweets in time windows, this will be our batches\n",
    "    group = (parse \n",
    "        | 'window' >> beam.WindowInto(beam.window.FixedWindows(10))\n",
    "        | 'group' >> beam.GroupBy()\n",
    "    )\n",
    "\n",
    "    def debug_fn(json_with_prediction: Dict) -> Dict:\n",
    "        \"\"\"Pretty print for model predictions\"\"\"\n",
    "        txt = json_with_prediction[\"text\"]\n",
    "\n",
    "        dots = re.findall('.{1,80}', ' '*len(txt))\n",
    "        txt = re.findall('.{1,80}', txt)\n",
    "\n",
    "        for txt_line, dots_line in zip(txt, dots):\n",
    "            if json_with_prediction[\"prediction\"] == \"NORMAL\":\n",
    "                ff = f\"{txt_line:80s} | {dots_line:80s}\"\n",
    "            else:\n",
    "                ff = f\"{dots_line:80s} | {txt_line:80s}\"\n",
    "            print(ff)\n",
    "\n",
    "        print(f\"{' '*80} | {' '*80}\")\n",
    "\n",
    "        return json_with_prediction\n",
    "    \n",
    "    # make predictions and converts them to JSON then to bytes\n",
    "    predict = (\n",
    "        group\n",
    "        | 'predict' >> beam.ParDo(ApplyModel())\n",
    "        | 'debug' >> beam.Map(debug_fn)\n",
    "        #| 'to_bytes' >> beam.Map(lambda x: json.dumps(x).encode(\"utf-8\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVVKI7fWUbKS"
   },
   "source": [
    "# How to run a pipeline\n",
    "\n",
    "## Locally\n",
    "\n",
    "We can run a Beam pipeline locally using the `DirectRunner`, it's useful to for debugging and testing.\n",
    "\n",
    "## Remotely\n",
    "In production, we want to run the pipeline into a remote executor for maximum performance and potentially scaling capabilities. \n",
    "In this workshop we will use the Google Cloud DataFlow executor. \n",
    "\n",
    "## How we proceed\n",
    "\n",
    "We will first run it locally to make sure that everything runs smoothly.\n",
    "\n",
    "Then we will run it on Google Cloud DataFlow runner to simulate a production application.\n",
    "\n",
    "> Disable warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "JB1PJsUlXcxG",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions, StandardOptions, GoogleCloudOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "mq6-AGz1KSpB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5e9c88f1-c2ef-41c7-fd61-0322274ed81b"
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(f\"{'NORMAL':80s} | {'DISASTER':80s}\")\n",
    "print(f\"{'':80s} | {'':80s}\")\n",
    "\n",
    "options = PipelineOptions()\n",
    "options.view_as(StandardOptions).streaming = True\n",
    "\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    build_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary snippet to launch our code in a real production environment\n",
    "\n",
    "As `google` is not defined in this case, the code will not run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "635AmXoqpN73",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set up Apache Beam pipeline options.\n",
    "options = PipelineOptions(streaming=True)\n",
    "\n",
    "# Set the project to the default project in your current Google Cloud\n",
    "# environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Set the Google Cloud region to run Dataflow.\n",
    "options.view_as(GoogleCloudOptions).region = 'europe-west6'\n",
    "\n",
    "# Choose a Cloud Storage location.\n",
    "dataflow_gcs_location = 'gs://bucket/dataflow'\n",
    "\n",
    "# Set the staging location. This location is used to stage the\n",
    "# Dataflow pipeline and SDK binary.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Set the temporary location. This location is used to store temporary files\n",
    "# or intermediate results before outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n",
    "\n",
    "# Set the SDK location. This is used by Dataflow to locate the\n",
    "# SDK needed to run the pipeline.\n",
    "#options.view_as(SetupOptions).sdk_location = (\n",
    "#    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' %\n",
    "#    beam.version.__version__)\n",
    "\n",
    "pipe = build_pipeline(beam.Pipeline())\n",
    "runner = DataflowRunner()\n",
    "runner.run_pipeline(pipe, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meOJi9BeK28i"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "## Machine Learning on Streams \n",
    "Processing streams with machine learning has many applications:\n",
    "\n",
    "1. IoT sensors\n",
    "2. Server logs ...\n",
    "\n",
    "\n",
    "## Apache Beam\n",
    "Apache Beam facilitate stream manipulation. It has numerous advantages:\n",
    "\n",
    "1. Multiple compatible backends\n",
    "2. Available in multiple programming languages\n",
    "3. Built-in support for batch and **streaming** processing\n",
    "\n",
    "\n",
    "## Serverless\n",
    "We used multiple managed services (Storage, Pub/Sub, DataFlow). It simple to setup and it scales smoothly.\n",
    "Just be careful with the bill, features like auto-scaling can surprise you.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZzuxZgFNnNd"
   },
   "source": [
    "# Thank You For Attending !\n",
    "\n",
    "## We hope this workshop will be helpful !"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
