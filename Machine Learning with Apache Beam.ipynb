{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0o_pH41bp6z"
   },
   "source": [
    "# Machine Learning on Streams with Apache Beam\n",
    "\n",
    "# What we will build \n",
    "\n",
    "In this workshop, we will learn how to use Apache Beam to detect potential disasters occurring in the world. We will receive a stream of Tweets as input, use machine learning to detect anomalies, and send alert events in an output stream. These alerts can be consumed in real-time by any service of interest.\n",
    "\n",
    "# Coding environment\n",
    "\n",
    "You simply need to install Apache Beam using `pip install apache-beam` to run this notebook. You also need pandas and scikit-learn.\n",
    "\n",
    "If you are using Noto, you can simply run these commands in your terminal\n",
    "\n",
    "```\n",
    "my_venvs_create dag\n",
    "my_venvs_activate dag\n",
    "pip install apache-beam\n",
    "my_kernels_create dag \"DAG\"\n",
    "```\n",
    "\n",
    "Then, switch your notebook kernel using the kernel selector on the top-right of this page. If it does not appear in the dropdown, close the window, shut down the notebook's kernel and re-open the notebook.\n",
    "\n",
    "# Streaming Data Processing\n",
    "\n",
    "Data processing is either offline or online. There are a lot of applications for streaming/online machine learning, where you receive a continuous flux of data to transform. The output can then be either stored or directly sent to another output stream.\n",
    "For instance, detecting anomalies in a continuous stream of events (IoT sensors, transactions, ...) and sending alterts to an output stream.\n",
    "\n",
    "![Batch vs. Stream Processing](assets/batch-stream.png)\n",
    "\n",
    "\n",
    "# Apache Beam\n",
    "\n",
    "**Apache Beam** is a unified programming model to define batch and **streaming** data processing jobs. Once you define a job, you need an execution engine, or runner, to run your batch. Beam is compatible with multiple runners such as *Apache Spark*, *Apache Flink* or *Google DataFlow*. It follows the philosopy \"Write once, run everywhere\" and has SDK in multiple languages, i.e **Python**, Java and Go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WujCPEIqJnaL"
   },
   "source": [
    "\n",
    "## Assumptions\n",
    "\n",
    "We assume a basic knowledge in machine learning (mainly Pandas and SKlearn) and Python programming language.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQIgef4OJ4q9"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset comes from a Kaggle competition: [Natural Language Processing with Disaster Tweets\n",
    "](https://www.kaggle.com/c/nlp-getting-started).\n",
    "\n",
    "The objective is to predict which Tweets talks about about natural disasters and which does not.\n",
    "\n",
    "Example of disaster Tweet:\n",
    "> Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
    "\n",
    "> Forest fire near La Ronge Sask. Canada\n",
    "\n",
    "> All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
    "\n",
    "Example of normal Tweet:\n",
    "> What's up man?\n",
    "\n",
    "> I love fruits\n",
    "\n",
    "> Summer is lovely\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m25DTXxUJ_A4"
   },
   "source": [
    "# Technologies\n",
    "\n",
    "Since we don't assume any access to cloud providers, we will run this notebook locally. Doing so, we will simulate some critical parts like streaming and storage. I will give you a quick overview of the services you could use in a production setup on the Google cloud. Other cloud providers have equivalent for all these technologies.\n",
    "\n",
    "1. **Google Cloud Storage**: bucket to store the data and the trained model (replaced with our local disk)\n",
    "2. **Apache Beam**: Python SDK to create data processing job\n",
    "3. **Google Cloud Pub/Sub**: ingestion platform for event-driven systems and streaming analytics (replaced with a simulated stream)\n",
    "4. **Google Cloud DataFlow**: cloud executor for job expressed with Apache Beam SDK (replaced with a local runner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TJBGkOrKE3a"
   },
   "source": [
    "\n",
    "# Overall System\n",
    "![overall pipeline](https://i.postimg.cc/sDSB2Tzm/overall-pipeline.png\n",
    "\"Overall System\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07PIO-DaKHrk"
   },
   "source": [
    "\n",
    "# Beam Pipeline\n",
    "\n",
    "![beam pipeline](https://i.postimg.cc/T1rRkTD9/beam-pipeline.png\n",
    "\"Beam Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytaY3jMyKQPj"
   },
   "source": [
    "# Plan\n",
    "\n",
    "1. Quick introduction on streams\n",
    "2. Train a simple model and save it locally\n",
    "3. Prepare the input stream (Tweets)\n",
    "4. Build a Beam streaming pipeline to process the Tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5sXeRTiggvd"
   },
   "source": [
    "# Setup and Configuration\n",
    "Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWOZQnsF88-6"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Iterable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znGc0CGh1YGl"
   },
   "source": [
    "# A glance at Google Cloud Pub/Sub\n",
    "\n",
    "It's the service used in this workshop to handle stream of events. We briefly introduce the 2 main concepts to understand how it works: **Topics** and **Subscriptions**.\n",
    "\n",
    "\n",
    "## Topics: enqueuing messages\n",
    "\n",
    "A topic can be seen as queue. It's the place where you push/enqueue messages. In our setup we want a topic to push Tweets (input) and a topic to push predictions (output).\n",
    "\n",
    "## Subscriptions: dequeuing (consuming) messages\n",
    "\n",
    "Now that messages are pushed to the topics (*queues*) we use subscriptions to pull messages out of it (*dequeue*). Any application who need to access message in the queue does it through a subscription. In our case we want a subscription to pull Tweets for the first queue and a subscription to pull predictions from the second queue.\n",
    "\n",
    "A topic can have multiple subscriptions. It will deliver the messages once to every subscriptions. \n",
    "\n",
    "\n",
    "\n",
    "## Overview\n",
    "![overall pipeline](https://i.postimg.cc/zvL1f2Rk/pub-sub.png\n",
    "\"Overall System\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MH7Y64xrjvKi"
   },
   "source": [
    "# Modeling\n",
    "\n",
    "We need a model to classify tweets as disaster or not. \n",
    "\n",
    "**The purpose of this workshop is not modeling, so we will not spend to much time on it and go for an easy solution. The pre-processing and modeling are purposedly rudimentary.**\n",
    "\n",
    "We build the model as a SKlearn pipeline, it has the advantage to bundle multiple steps into one estimator. We can then have our preprocessing included in the model:\n",
    "\n",
    "1. First we select the column of interest in the DataFrame, in our case the text column. This column does not contain any NA values so ...\n",
    "2. We vectorize the raw text using TFIDF ([TFIDF details](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), [Sklearn TFIDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))\n",
    "3. We apply a Random Forest classifier ([Sklearn RandomForestClassfier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Here are two links for the train and test datasets:\n",
    "\n",
    "- [Train dataset](https://drive.google.com/file/d/1rsrAu4F13UCHsKpWjxRIh0ObsjWSMVSE/view?usp=sharing)\n",
    "- [Test dataset](https://drive.google.com/file/d/1yjX4e2U2auLQn01HYUgBKEJT8q3g15BJ/view?usp=sharing)\n",
    "\n",
    "Download them and place them in the sames folder as the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g41_Y6Tw1Yg6"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "wRiNljO_JLM_",
    "outputId": "d30129ad-040f-467e-926d-4d7972fbfc52"
   },
   "outputs": [],
   "source": [
    "# Read the data from our Google Cloud bucket\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "df_train.dropna().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppr3pUCkYxzI"
   },
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uR-BLx31lqm"
   },
   "outputs": [],
   "source": [
    "def select_text(df: pd.DataFrame) -> pd.Series:\n",
    "    return df[\"text\"]\n",
    "\n",
    "# Model pipeline\n",
    "pipe = Pipeline([\n",
    "    ('selector', FunctionTransformer(select_text)),\n",
    "    ('tfidf', TfidfVectorizer(lowercase=True)),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "UxC7kcGf6EgF",
    "outputId": "5a1343aa-8c32-485e-b799-4cd1be12b5b8"
   },
   "outputs": [],
   "source": [
    "# Cross-validation step to get an idea of model performances\n",
    "\n",
    "cv_results = cross_validate(pipe, df_train, df_train['target'], cv=3, scoring=[\"roc_auc\"], return_train_score=True, n_jobs=-1)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results.agg((\"mean\", \"std\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baIg4s8s_fuN"
   },
   "outputs": [],
   "source": [
    "model = pipe.fit(df_train, df_train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "bHov_EhOl0dQ",
    "outputId": "e9a997bb-0100-4d66-9b8f-5edbc269fb3b"
   },
   "outputs": [],
   "source": [
    "# We pickle and save the model\n",
    "pickle.dump(model, open(\"model.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-3JqE8dcOWz"
   },
   "source": [
    "# Beam Pipeline Overview\n",
    "\n",
    "We can summarize beam pipelines as follow:\n",
    "\n",
    "1. Read data from any source into a `PCollection` A\n",
    "2. Transform A into another `PCollection` B by applying a `PTranform` (map, filter, ...)\n",
    "3. Transform B intot another `PCollection`\n",
    "4. ... keep chaining tranformers\n",
    "5. Save the final output somewhere\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![beam-pipeline-doc](https://beam.apache.org/images/design-your-pipeline-linear.svg \"Windowing\")\n",
    "[*Image taken from official documentation*](https://beam.apache.org/documentation/programming-guide/#transforms)\n",
    "\n",
    "<br/>\n",
    "\n",
    "## Initialization\n",
    "\n",
    "We first need an pipeline. It will be the entry point:\n",
    "\n",
    "```python\n",
    "pipeline = beam.Pipeline()\n",
    "```\n",
    "\n",
    "Or to create a pipeline and run it locally directly:\n",
    "\n",
    "```python\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    ...\n",
    "```\n",
    "\n",
    "## Applying a transform on your pipeline\n",
    "\n",
    "Use the **pipe** operator `|` to apply a transform on a collection. You can pair it with the `>>` to give a name to the transformation:\n",
    "\n",
    "Without name:\n",
    "\n",
    "```python\n",
    "pipeline | transform\n",
    "```\n",
    "\n",
    "With name:\n",
    "\n",
    "```python\n",
    "pipeline | \"step_name\" >> transform\n",
    "```\n",
    "\n",
    "## Read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbzxmnoywjUF"
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VurOXSSLm27y",
    "outputId": "ffddaa42-b857-4e28-e7b7-650eb6f25996"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    (pipeline\n",
    "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvzwqOeVmjyD"
   },
   "source": [
    "\n",
    "## Common operators\n",
    "\n",
    "### beam.Map (map a function on a collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DeLzIK1UlXod",
    "outputId": "ae7f7a40-f4c3-483a-f1c0-5ca1bbdae32f"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    (pipeline\n",
    "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"add_one\" >> beam.Map(lambda x: x + 1)\n",
    "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcw2t0J8keRH"
   },
   "source": [
    "### beam.ParDo with beam.DoFn\n",
    "\n",
    "Beam has a generic transform for data processing: `beam.ParDo`. \n",
    "It takes as input a function to apply: `beam.DoFn`. \n",
    "\n",
    "### ParDo\n",
    "1. run on each element in the input `PCollection`\n",
    "2. apply a processing function\n",
    "3. emit **zero**, **one**, or **many outputs** for each input element\n",
    "\n",
    "It's generic and can implement any usual transform: map, flatmap, filter, ...\n",
    "Indeed all these are special cases of `beam.ParDo` processing.\n",
    "\n",
    "\n",
    "#### DoFn\n",
    "`beam.DoFn` represent a processing function applied by the `beam.ParDo`.\n",
    "\n",
    "A beam.DoFn function **should always return an iterable or None**. beam.ParDo will flatten the iterable.\n",
    "For instance if you want to return one element, you need to wrap it into an iterable.\n",
    "The first dimension of your output is flattened. \n",
    "\n",
    "Another nice feature is **stateful computation**. Since `beam.DoFn` is a class, it is possible to have an **internal state**. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LwAxRlf9c9z"
   },
   "source": [
    "Quick example: add an incremental index, starting from N, to elements\n",
    "\n",
    "```\n",
    "Input: 1, 2, 3, 4, 5\n",
    "\n",
    "Apply IdxFn\n",
    "\n",
    "Output:  (N, 1), (N+1, 2), (N+2, 3), (N+3, 4), (N+4, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EnDVvN6lkevQ",
    "outputId": "6e269a40-09e0-4229-dc4d-facec2c68d74"
   },
   "outputs": [],
   "source": [
    "class IdxFn(beam.DoFn):\n",
    "    \"\"\"Custom DoFn function to add an index. It will be applied using beam.ParDo\"\"\"\n",
    "    def __init__(self, init_state: int):\n",
    "        # this is is the first number to use as an index, we will increment it for each element\n",
    "        self.state = init_state\n",
    "\n",
    "    def process(self, elem: Any) -> Iterable[Any]:\n",
    "        # create a tuple with the current state as index and the element\n",
    "        res = (self.state, elem)\n",
    "        # increment the state\n",
    "        self.state += 1\n",
    "        return [res]\n",
    "\n",
    "print(\"Like a Map:\")\n",
    "with beam.Pipeline() as pipeline:\n",
    "    (pipeline\n",
    "        | \"read_data\" >> beam.Create([1, 2, 3, 4, 5])\n",
    "        | \"with_idx\" >> beam.ParDo(IdxFn(init_state=10))\n",
    "        | \"print\" >> beam.Map(lambda x: print(x, end=', '))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcLlyyyWlpIy"
   },
   "source": [
    "### beam.Filter (filter a collection) - Optional, not used in this workshop\n",
    "\n",
    "Write a similar pipeline that prints only the even numbers in a created list of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVgVTUs5ipIz",
    "outputId": "b7ff7f81-d469-4a0a-a45b-cd5741fce473"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    ### \n",
    "    ### YOUR CODE HERE\n",
    "    ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWSvhW3_j4xe"
   },
   "source": [
    "### beam.window.WindowInto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-XeoYLPwgDF"
   },
   "source": [
    "#### Without window\n",
    "\n",
    "![without-window](https://i.postimg.cc/N0Nksgrx/without-window.png\n",
    "\"Windowing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htCT4SRawmhf"
   },
   "source": [
    "#### With session window\n",
    "\n",
    "![with-window](https://i.postimg.cc/cLFWzjCV/with-window.png\n",
    "\"Windowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLu3MEbaerc5",
    "outputId": "2bf23fbd-b51b-4d83-ed8a-5448e344990d"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    input_stream = [(1, 100), (2, 101), (3, 102), (4, 200)]\n",
    "    input_stream = [beam.window.TimestampedValue(elem, timestamp) for elem, timestamp in input_stream]\n",
    "    pc = (pipeline\n",
    "        | beam.Create(input_stream)\n",
    "        | 'window' >> beam.WindowInto(beam.window.FixedWindows(5))\n",
    "        | 'group' >> beam.GroupBy()\n",
    "        | 'pp' >> beam.Map(lambda x: print(f\"Batch: {x}\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-9a_g1VI4U6"
   },
   "source": [
    "# Our Beam Pipeline\n",
    "\n",
    "As a reminder, this is what we want:\n",
    "\n",
    "![beam pipeline](https://i.postimg.cc/xT5TvZnJ/beam-pipeline-focus.png\n",
    "\"Beam Pipeline\")\n",
    "\n",
    "We will focus on two important parts: **Grouping by chunk** of time and **Predicting**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teGW6YTwJ3jy"
   },
   "source": [
    "# Predict\n",
    "\n",
    "The most custom and critical step here is the **prediction**. Indeed we need to build a custom `beam.DoFn` with an initialization step pulling the pickled model from Google Cloud and loading the model. Then use the model to transform the incoming tweets. \n",
    "\n",
    "Here are the steps for the prediction part:\n",
    "1. we implement a custom `beam.DoFn` class called `ApplyModel`\n",
    "2. in the `__init__` method we pull and load the model\n",
    "3. we implement the `process` method which receives batch of tweets and use the model to label them as *DISASTER* or *NORMAL*\n",
    "\n",
    "\n",
    "![dofn](https://i.postimg.cc/KYpdxdBT/dofn.png\n",
    "\"Windowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRb9F-MPG9vp"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a simulated Tweet stream (from the testing set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jsons = pd.read_csv(\"data/test.csv\").to_dict(\"records\")\n",
    "\n",
    "timestamps = list(range(0, 10)) + list(range(50, 60)) \n",
    "tweets_input_stream = list(zip(test_jsons[:len(timestamps)], timestamps))\n",
    "\n",
    "tweets_input_stream = [beam.window.TimestampedValue(elem, timestamp) for elem, timestamp in tweets_input_stream]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply model in Beam pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyModel(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the function\"\"\"\n",
    "        self._model = None\n",
    "        # We import in the init statement to have the library available even when we run the pipeline \n",
    "        # on remote executors such as DataFlow\n",
    "        import pandas as pd\n",
    "        import pickle\n",
    "        self._pd = pd\n",
    "        self._model = pickle.load(open(\"model.pkl\", 'rb'))\n",
    "\n",
    "     \n",
    "    def process(self, group: Tuple[Any, Iterable[Any]]) -> Iterable[Any]:\n",
    "        \"\"\"Process every batch of tweets\"\"\"\n",
    "        # extract batch and build DataFrame\n",
    "        \n",
    "        ### \n",
    "        ### YOUR CODE HERE\n",
    "        ###\n",
    "        \n",
    "        # predict\n",
    "        # modify \"prediction\" column in df as \"NORMAL\" for 0 and \"DISASTER\" for 1\n",
    "        # format DataFrame to JSON format in variable `res`\n",
    "        \n",
    "        ### \n",
    "        ### YOUR CODE HERE\n",
    "        ###\n",
    "    \n",
    "        return res\n",
    "\n",
    "\n",
    "def build_pipeline(pipeline: beam.Pipeline) -> beam.Pipeline:\n",
    "    \"\"\"Takes an empty beam.Pipeline in input and returns the full beam.Pipeline\"\"\"\n",
    "\n",
    "    parse = pipeline | beam.Create(tweets_input_stream)\n",
    "\n",
    "    \n",
    "    # group the tweets in time windows, this will be our batches\n",
    "    group = (parse \n",
    "        | 'window' >> beam.WindowInto(beam.window.FixedWindows(10))\n",
    "        | 'group' >> beam.GroupBy()\n",
    "    )\n",
    "\n",
    "    def debug_fn(json_with_prediction: Dict) -> Dict:\n",
    "        \"\"\"Pretty print for model predictions\"\"\"\n",
    "        txt = json_with_prediction[\"text\"]\n",
    "\n",
    "        dots = re.findall('.{1,80}', ' '*len(txt))\n",
    "        txt = re.findall('.{1,80}', txt)\n",
    "\n",
    "        for txt_line, dots_line in zip(txt, dots):\n",
    "            if json_with_prediction[\"prediction\"] == \"NORMAL\":\n",
    "                ff = f\"{txt_line:80s} | {dots_line:80s}\"\n",
    "            else:\n",
    "                ff = f\"{dots_line:80s} | {txt_line:80s}\"\n",
    "            print(ff)\n",
    "\n",
    "        print(f\"{' '*80} | {' '*80}\")\n",
    "\n",
    "        return json_with_prediction\n",
    "    \n",
    "    # make predictions and converts them to JSON then to bytes\n",
    "    predict = (\n",
    "        group\n",
    "        | 'predict' >> beam.ParDo(ApplyModel())\n",
    "        | 'debug' >> beam.Map(debug_fn)\n",
    "        #| 'to_bytes' >> beam.Map(lambda x: json.dumps(x).encode(\"utf-8\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVVKI7fWUbKS"
   },
   "source": [
    "# How to run a pipeline\n",
    "\n",
    "## Locally\n",
    "\n",
    "We can run a Beam pipeline locally using the `DirectRunner`, it's useful to for debugging and testing.\n",
    "\n",
    "## Remotely\n",
    "In production, we want to run the pipeline into a remote executor for maximum performance and potentially scaling capabilities. \n",
    "In this workshop we will use the Google Cloud DataFlow executor. \n",
    "\n",
    "## How we proceed\n",
    "\n",
    "We will first run it locally to make sure that everything runs smoothly.\n",
    "\n",
    "Then we will run it on Google Cloud DataFlow runner to simulate a production application.\n",
    "\n",
    "> Disable warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JB1PJsUlXcxG"
   },
   "outputs": [],
   "source": [
    "from apache_beam.runners import DataflowRunner\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions, StandardOptions, GoogleCloudOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mq6-AGz1KSpB",
    "outputId": "5e9c88f1-c2ef-41c7-fd61-0322274ed81b"
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(f\"{'NORMAL':80s} | {'DISASTER':80s}\")\n",
    "print(f\"{'':80s} | {'':80s}\")\n",
    "\n",
    "options = PipelineOptions()\n",
    "options.view_as(StandardOptions).streaming = True\n",
    "\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "    build_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary snippet to launch our code in a real production environment\n",
    "\n",
    "As `google` is not defined in this case, the code will not run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "635AmXoqpN73",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```python\n",
    "# Set up Apache Beam pipeline options.\n",
    "options = PipelineOptions(streaming=True)\n",
    "\n",
    "# Set the project to the default project in your current Google Cloud\n",
    "# environment.\n",
    "_, options.view_as(GoogleCloudOptions).project = google.auth.default()\n",
    "\n",
    "# Set the Google Cloud region to run Dataflow.\n",
    "options.view_as(GoogleCloudOptions).region = 'europe-west6'\n",
    "\n",
    "# Choose a Cloud Storage location.\n",
    "dataflow_gcs_location = 'gs://bucket/dataflow'\n",
    "\n",
    "# Set the staging location. This location is used to stage the\n",
    "# Dataflow pipeline and SDK binary.\n",
    "options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location\n",
    "\n",
    "# Set the temporary location. This location is used to store temporary files\n",
    "# or intermediate results before outputting to the sink.\n",
    "options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location\n",
    "\n",
    "# Set the SDK location. This is used by Dataflow to locate the\n",
    "# SDK needed to run the pipeline.\n",
    "#options.view_as(SetupOptions).sdk_location = (\n",
    "#    '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' %\n",
    "#    beam.version.__version__)\n",
    "\n",
    "pipe = build_pipeline(beam.Pipeline())\n",
    "runner = DataflowRunner()\n",
    "runner.run_pipeline(pipe, options)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meOJi9BeK28i"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "## Machine Learning on Streams \n",
    "Processing streams with machine learning has many applications:\n",
    "\n",
    "1. IoT sensors\n",
    "2. Server logs ...\n",
    "\n",
    "\n",
    "## Apache Beam\n",
    "Apache Beam facilitate stream manipulation. It has numerous advantages:\n",
    "\n",
    "1. Multiple compatible backends\n",
    "2. Available in multiple programming languages\n",
    "3. Built-in support for batch and **streaming** processing\n",
    "\n",
    "\n",
    "## Serverless\n",
    "We used multiple managed services (Storage, Pub/Sub, DataFlow). It simple to setup and it scales smoothly.\n",
    "Just be careful with the bill, features like auto-scaling can surprise you.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZzuxZgFNnNd"
   },
   "source": [
    "# Thank You For Attending !\n",
    "\n",
    "## We hope this workshop will be helpful !"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
